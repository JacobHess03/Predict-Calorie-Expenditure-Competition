Predict-Calorie-Expenditure-Competition
Description

This repository contains the code developed for the Kaggle "Predict Calorie Expenditure" competition. It presents two distinct approaches to tackle the problem of calorie expenditure regression and prediction based on the provided data:

    A traditional Machine Learning pipeline based on XGBoost with an extensive phase of feature engineering, preprocessing, and hyperparameter optimization via automatic search. (main.py)
    A Deep Learning model based on the Wide & Deep architecture implemented with TensorFlow/Keras, which includes normalization layers, skip-connections, and the use of callbacks for efficient training on CPU/GPU. (mainDL.py)

The goal is to compare the performance of a boosted tree-based model against a neural model on this specific regression task.
Repository Structure
Plaintext

    ├── data/
    │   ├── train.csv             # Training data (with Calories target)
    │   └── test.csv              # Test data (without target)
    ├── main.py                   # ML script (XGBoost pipeline)
    ├── mainDL.py                 # Deep Learning script (Keras Wide & Deep)
    ├── submission.csv            # Example submission file generated by main.py
    ├── submission_dl.csv         # Example submission file generated by mainDL.py
    ├── requirements.txt          # List of Python dependencies
    └── README.md                 # This file

Requirements

    Python 3.8+
    The libraries listed in requirements.txt

Installation

It's highly recommended to use a virtual environment to install the dependencies:
Bash

# Create a virtual environment (if you don't have one already)
python -m venv venv

# Activate the virtual environment
# On macOS/Linux:
source venv/bin/activate
# On Windows:
venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

The requirements.txt file contains the following dependencies:

pandas>=1.3
numpy>=1.21
scikit-learn>=1.0
xgboost>=1.6
matplotlib>=3.4
tensorflow>=2.8

Ensure that train.csv and test.csv are placed in the data/ directory before running the scripts.
main.py Script – XGBoost Pipeline

This script implements a complete Machine Learning pipeline using the XGBoost model.
Key Features:
    
    Data Loading: Reads train.csv and test.csv, preserving IDs.
    Feature Engineering:
        Calculates BMI (Weight / Height^2).
        Creates categorical Age_Group (0-18, 19-40, 41-65, 66-100).
    Preprocessing:
        Encodes Sex with LabelEncoder.
        (Optional) Outlier removal (z-score).
        Numerical transformations: StandardScaler, PowerTransformer, PolynomialFeatures (degree 2, interactions only).
        (Optional) Log-transform of the Calories target.
    Model Optimization: Performs RandomizedSearchCV (XGBoost, 15 iterations, CV=3) for hyperparameter tuning.
    Final Training: Trains the best model on the entire training set.
    Prediction & Submission: Generates predictions on the test set and creates submission.csv.

Execution:

    Bash
    
    python main.py

Expected Output:

    In console: optimal parameters, validation metrics (R², RMSE).
    Generated file: submission.csv.

    mainDL.py Script – Wide & Deep Model in Keras

This script uses TensorFlow/Keras to build and train a Deep Learning model with a Wide & Deep architecture.
Key Features:

    Data Loading: Reads train.csv and test.csv, preserving IDs.
    Feature Engineering:
        Calculates BMI.
        Creates Age_Group.
    Preprocessing:
        Encodes Sex.
        Train/Validation split (80%/20%).
        Numerical data normalization with layers.Normalization() (adapted on training data).
    Model Architecture (Wide & Deep):
        Normalized input.
        Deep Path: Dense(64, ReLU) → Dense(32, ReLU).
        Skip connection: concatenation (Normalized Input + Deep Path output).
        Output: Dense(1).
    Compilation: Loss mse, Optimizer Adam (lr=1e-3), Metric RootMeanSquaredError.
    Callbacks: EarlyStopping (patience=10, val_loss), TensorBoard.
    Training: Trains up to 100 epochs with EarlyStopping and validation.
    Visualization: Plots training/validation curves with Matplotlib.
    Prediction & Submission: Generates predictions on the test set and creates submission_dl.csv.

Execution:

    Bash
    
    python mainDL.py

Expected Output:

    In console: model.summary(), training progress per epoch (loss, RMSE).
    Generated file: submission_dl.csv.

Graph: Training/Validation Curves


![mainDL](https://github.com/user-attachments/assets/9cb1cb4c-287c-46b4-a073-1efc6154e29f)

